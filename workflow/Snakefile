from snakemake.utils import min_version

min_version("6.10.0")

# Configuration file containing all user-specified settings
configfile: "config/config.yml"

OUTDIR="data/reads"  

SAMPLES = list(set(glob_wildcards(os.path.join('data/reads', '{sample}_{readNum, R[12]}_001.fastq.gz')).sample))
# SAMPLES = list(set(glob_wildcards(os.path.join('data/reads', '{sample}_{sradNum, [12]}.fastq.gz')).sample))

rule FINAL_TARGET:
    input:
        "docs/index.html",
        "reports/snakemake_report.html",
        "reports/project_tree.txt",
        "dags/rulegraph.png",
        "dags/rulegraph.svg",   
        "data/references/silva.v4.align",
        "data/references/trainset16_022016.pds.fasta",
        "data/references/trainset16_022016.pds.tax" 

        # expand("{outdir}/{sample}_{sraNum}.fastq.gz", outdir=OUTDIR, sample=SAMPLES, sraNum=config["sraNum"]),
        # "results/qc/seqkit1/seqkit_stats.txt",        
        # expand("results/qc/fastqc1/{sample}_{sraNum}_fastqc.html", sample=SAMPLES, sraNum=config["sraNum"]),
        # expand("results/qc/fastqc1/{sample}_{sraNum}_fastqc.zip", sample=SAMPLES, sraNum=config["sraNum"]),
        # "results/qc/multiqc1/multiqc_report.html", 
        # "results/qc/multiqc1/multiqc_plots/svg/mqc_fastqc_per_base_sequence_quality_plot_1.svg",

        # expand("resources/reads/trimmed/{sample}_{sraNum}.fastq.gz", sample=SAMPLES, sraNum=config["sraNum"]),
        # "results/qc/seqkit2/seqkit_stats.txt",        
        # expand("results/qc/fastqc2/{sample}_{sraNum}_fastqc.html", sample=SAMPLES, sraNum=config["sraNum"]),
        # expand("results/qc/fastqc2/{sample}_{sraNum}_fastqc.zip", sample=SAMPLES, sraNum=config["sraNum"]),
        # "results/qc/multiqc2/multiqc_report.html", 
        # "results/qc/multiqc2/multiqc_plots/svg/mqc_fastqc_per_base_sequence_quality_plot_1.svg",

        # expand("resources/reads/trimmed/decontam/{sample}_{sraNum}.fastq.gz", sample=SAMPLES, sraNum=config["sraNum"]),
        # "results/qc/seqkit3/seqkit_stats.txt",        
        # expand("results/qc/fastqc3/{sample}_{sraNum}_fastqc.html", sample=SAMPLES, sraNum=config["sraNum"]),
        # expand("results/qc/fastqc3/{sample}_{sraNum}_fastqc.zip", sample=SAMPLES, sraNum=config["sraNum"]),
        # "results/qc/multiqc3/multiqc_report.html",
        # "results/qc/multiqc3/multiqc_plots/svg/mqc_fastqc_per_base_sequence_quality_plot_1.svg",

        # "images/samples_hist.svg",
        # "images/qc_hist.svg",

        # "resources/qiime2_manifest_file.tsv",
        # "resources/qiime2_sample_metadata.tsv",

        # "resources/mothur_mapping_file.tsv",
        # "resources/mothur_sample_metadata.tsv",

        # "config/mothur/samples.tsv",
        # "config/mothur/units.tsv",
        # "config/qiime2/samples.tsv",
        # "config/qiime2/units.tsv",

        # "dags/rulegraph.svg",
        # "images/smkreport/screenshot.png",
        # "images/project_tree.txt",


 
# Rule to generate the rule graph and create the dags folder if it doesn't exist
rule generate_rulegraph:
    output:
        directory("dags"),
        "dags/rulegraph.png",
        "dags/rulegraph.svg",

    shell:
        """
        snakemake --unlock;
        mkdir -p {output[0]};
        snakemake --rulegraph | dot -Tpng > {output[1]};
        snakemake --rulegraph | dot -Tsvg > {output[2]};
        """

# Rule to generate a directory tree
rule project_tree:
    output:
        directory("output"),
        "reports/project_tree.txt"
    shell:
        """
        snakemake --unlock \
        && mkdir -p {output[0]} \
        && echo "Project Tree" > {output[1]} \
        && tree -L 2 . >> {output[1]}
        """

# Rule to generate SnakeMake report
rule snakemake_report:
    output:
        "reports/snakemake_report.html"
    shell:
        "snakemake --keep-going --use-conda --report {output[0]}"


#####################################
#####################################
#####################################
#####################################

rule import_fastq_reads:
    output:
        expand('{outdir}/{sample}_{readNum}_001.fastq.gz', outdir=OUTDIR, sample=SAMPLES, readNum=config["readNum"]),
    shell:
        "bash workflow/scripts/import_data.sh"


rule import_mothur_metadata:
    input:
        "config/units.tsv"
    output:
        "data/metadata/test.files"
    shell:
        "bash workflow/scripts/import_data.sh"


# Downloading and formatting SILVA and RDP reference databases. The v4 region is extracted from 
# SILVA database for use as reference alignment.
rule download_mothur_refs:
	output:
		silvaV4="data/references/silva.v4.align",
		rdpFasta="data/references/trainset16_022016.pds.fasta",
		rdpTax="data/references/trainset16_022016.pds.tax"
	conda:
		"../envs/mothur.yml"
	shell:
		"bash workflow/scripts/mothurReferences.sh"


# Downloading the Zymo mock sequence files and extracting v4 region for error estimation.
rule get_mothur_zymo_mock:
	input:
		script="workflow/scripts/mothurMock.sh",
		silvaV4="data/references/silva.v4.align",
	output:
		"data/references/zymo.mock.16S.v4.fasta"
	conda:
		"../envs/mothur.yml"
	shell:
		"bash {input.script}"



# # Generating master OTU shared file.

# # Making mothur-based sample mapping file.
# rule mothur_generated_mapping:
#     input:
#         script="workflow/scripts/makeFile.sh",
#         reads=expand('{outdir}/{sample}_{readNum}_001.fastq.gz', outdir=OUTDIR, sample=SAMPLES, readNum=config["readNum"])
#     output:
#         expand("mothur_process/{dataset}.files", dataset=config["dataset"]),
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash {input.script}"



# rule mothur_process_sequences:
#     input:
#         "data/metadata/test.files",
#         expand("mothur_process/{dataset}.files", dataset=config["dataset"]),
#         script="workflow/scripts/mothur_process_seqs.sh",
#         silvaV4="data/references/silva.v4.align",
#         rdpFasta="data/references/trainset16_022016.pds.fasta",
#         rdpTax="data/references/trainset16_022016.pds.tax",
#     output:
#         fasta="mothur_process/final.fasta",
#         ctable="mothur_process/final.count_table",
#         taxonomy="mothur_process/final.taxonomy",
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash {input.script}"


# # Preparing final processed fasta and count table.
# rule mothur_final_processed_seqs:
# 	input:
# 		script="workflow/scripts/mothur_final_processed_seqs.sh",
# 		metadata=rules.mothur_process_sequences.output
# 	output:
# 		fasta="mothur_process/{method}/final.fasta",
# 		ctable="mothur_process/{method}/final.count_table",
# 		taxonomy="mothur_process/{method}/final.taxonomy",
# 	conda:
# 		"../envs/mothur.yml"
# 	shell:
# 		"bash {input.script}"


# # Classify OTUS
# rule mothur_classify_otus:
#     input:
#         script="workflow/scripts/mothur_classify_otus.sh",
#         infiles=expand(rules.mothur_final_processed_seqs.output, method="otu_analysis"),
#     output:
#         dist=expand("mothur_process/{method}/final.dist", method="otu_analysis"),
#         fasta=expand("mothur_process/{method}/final.opti_mcc.0.03.rep.fasta", method="otu_analysis"),
#         mcclist=expand("mothur_process/{method}/final.opti_mcc.list", method="otu_analysis"),
#         shared=expand("mothur_process/{method}/final.opti_mcc.shared", method="otu_analysis"),
#         taxonomy=expand("mothur_process/{method}/final.pds.wang.pick.taxonomy", method="otu_analysis"),
#         constaxonomy=expand("mothur_process/{method}/final.opti_mcc.0.03.cons.taxonomy", method="otu_analysis"),
#         ctable=expand("mothur_process/{method}/final.opti_mcc.0.03.rep.count_table", method="otu_analysis"),
#         biom=expand("mothur_process/{method}/final.opti_mcc.0.03.biom", method="otu_analysis"),
#         lefse=expand("mothur_process/{method}/final.opti_mcc.0.03.lefse", method="otu_analysis"),
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash {input.script}"


# # Classify Phylotypes
# rule mothur_classify_phylotype:
#     input:
#         script="workflow/scripts/mothur_classify_phylotypes.sh",
#         infiles=expand(rules.mothur_final_processed_seqs.output, method="phylotype_analysis"),
#     output:
#         txlist=expand("mothur_process/{method}/final.pds.wang.pick.tx.list", method="phylotype_analysis"),
#         rabund=expand("mothur_process/{method}/final.pds.wang.pick.tx.rabund", method="phylotype_analysis"),
#         sabund=expand("mothur_process/{method}/final.pds.wang.pick.tx.sabund", method="phylotype_analysis"),
#         shared=expand("mothur_process/{method}/final.pds.wang.pick.tx.shared", method="phylotype_analysis"),
#         taxonomy=expand("mothur_process/{method}/final.pds.wang.pick.taxonomy", method="phylotype_analysis"),
#         constaxonomy=expand("mothur_process/{method}/final.pds.wang.pick.tx.1.cons.taxonomy", method="phylotype_analysis"),
#         biom=expand("mothur_process/{method}/final.pds.wang.pick.tx.1.lefse", method="phylotype_analysis"),
#         lefse=expand("mothur_process/{method}/final.pds.wang.pick.tx.1.biom", method="phylotype_analysis"),
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash {input.script}"


#     # Classify Phylotypes
# rule mothur_classify_asvs:
#     input:
#         script="workflow/scripts/mothur_classify_asvs.sh",
#         infiles=expand(rules.mothur_final_processed_seqs.output, method="asv_analysis"),
#     output:
#         asvlist=expand("mothur_process/{method}/final.asv.list", method="asv_analysis"),
#         shared=expand("mothur_process/{method}/final.asv.shared", method="asv_analysis"),
#         constaxonomy=expand("mothur_process/{method}/final.asv.ASV.cons.taxonomy", method="asv_analysis"),
#         biom=expand("mothur_process/{method}/final.asv.ASV.biom", method="asv_analysis"),
#         lefse=expand("mothur_process/{method}/final.asv.ASV.lefse", method="asv_analysis"),
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash {input.script}"


#     # Classify Phylotypes
# rule mothur_classify_phylogeny:
#     input:
#         script="workflow/scripts/mothur_classify_phylogeny.sh",
#         infiles=expand(rules.mothur_final_processed_seqs.output, method="phylogeny_analysis"),
#     output:
#         phylip=expand("mothur_process/{method}/final.phylip.dist", method="phylogeny_analysis"),
#         taxonomy=expand("mothur_process/{method}/final.pds.wang.pick.taxonomy", method="phylogeny_analysis"),
#         tree=expand("mothur_process/{method}/final.phylip.tre", method="phylogeny_analysis"),
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash {input.script}"

# # Calculate estimated sequencing error rate based on mock sequences.
# rule mothur_error_rate:
#     input:
#         script="workflow/scripts/mothur_error_rate.sh",
#         infiles=expand(rules.mothur_final_processed_seqs.output, method="error_analysis"),
#         mockV4=rules.get_mothur_zymo_mock.output.mockV4
#     output:
#         expand("mothur_process/{method}/final.pick.error.summary", method="error_analysis"),
#         expand("mothur_process/{method}/final.pick.error.seq", method="error_analysis"),
#         expand("mothur_process/{method}/final.pick.error.chimera", method="error_analysis"),
#         expand("mothur_process/{method}/final.pick.error.seq.forward", method="error_analysis"),
#         expand("mothur_process/{method}/final.pick.error.seq.reverse", method="error_analysis"),
#         expand("mothur_process/{method}/final.pick.error.count", method="error_analysis"),
#         expand("mothur_process/{method}/final.pick.error.matrix", method="error_analysis"),
#         expand("mothur_process/{method}/final.pick.error.ref", method="error_analysis"),
#     params:
#         mockGroups='-'.join(config["mothurMock"]) # Concatenates all mock group names with hyphens
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash {input.script} {input.infiles} {input.mockV4} {params.mockGroups}"



# # Splitting master shared file into individual shared file for: i) samples, ii) controls, and iii) mocks.
# # This is used for optimal subsampling during downstream steps.
# rule mothur_final_otutable:
# 	input:
# 		shared=rules.mothur_classify_otus.output
# 	output:
# 		shared=expand("mothur_process/{method}/{group}.final.shared", method="otu_analysis", group=config["mothurGroups"])
# 	params:
# 		mockGroups='-'.join(config["mothurMock"]), # Concatenates all mock group names with hyphens
# 		controlGroups='-'.join(config["mothurControl"]) # Concatenates all control group names with hyphens
# 	conda:
# 		"../envs/mothur.yml"
# 	shell:
# 		"bash workflow/scripts/mothur_split_otutable.sh"


# # # Diversity Metrics 

# # rule otu_diversity_analysis:
# #     input:
# #         shared=rules.mothur_final_otutable.output
# #     output:
# #         "mothur_process/otu_analysis/sample.final.count.summary",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.shared",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.groups.rarefaction",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.groups.summary",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.sharedsobs.0.03.lt.dist",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.braycurtis.0.03.lt.dist",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.braycurtis.0.03.lt.tre",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.braycurtis.0.03.lt.pcoa.axes",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.braycurtis.0.03.lt.pcoa.loadings",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.braycurtis.0.03.lt.nmds.iters",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.braycurtis.0.03.lt.nmds.stress",
# #         "mothur_process/otu_analysis/sample.final.0.03.subsample.braycurtis.0.03.lt.nmds.axes",

# #     conda:
# #         "../envs/mothur.yml"
# #     shell:
# #         "bash workflow/scripts/mothur_diversity_analysis.sh",


# # Count classified reads per sample 
# rule group_read_count:
#     input:
#         shared=rules.mothur_final_otutable.output
#     output:
#         "mothur_process/otu_analysis/sample.final.count.summary",
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash workflow/scripts/count_reads_per_sample.sh"


# rule subsample_otutable:
#     input:
#         shared=rules.mothur_final_otutable.output
#     output:
#         "mothur_process/otu_analysis/sample.final.0.03.subsample.shared",
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash workflow/scripts/subsample_otutable.sh"


# rule read_rarefaction:
#     input:
#         shared=rules.mothur_final_otutable.output
#     output:
#         "mothur_process/otu_analysis/sample.final.groups.rarefaction",
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash workflow/scripts/rarefaction.sh"


# # Alpha diversity Metrics
# rule alpha_diversity_metrics:
#     input:
#         shared=rules.subsample_otutable.output
#     output:
#         "mothur_process/otu_analysis/sample.final.0.03.subsample.groups.ave-std.summary",
#     params:
#         subthresh=config["subthresh"],
#         alpha='-'.join(config["mothurAlpha"])
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash workflow/scripts/alphadiv_metrics.sh {params}"

# # Beta diversity Metrics
# rule beta_diversity_metrics:
#     input:
#         shared=rules.subsample_otutable.output
#     output:
#         "mothur_process/otu_analysis/sample.final.0.03.subsample.sharedsobs.0.03.lt.dist",
#         "mothur_process/otu_analysis/sample.final.0.03.subsample.braycurtis.0.03.lt.dist",
#         "mothur_process/otu_analysis/sample.final.0.03.subsample.braycurtis.0.03.lt.tre",
#     params:
#         beta='-'.join(config["mothurBeta"])
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash workflow/scripts/betadiv_dist_n_tree.sh {params}"


# rule pcoa_mds_matrix:
#     input:
#         shared=rules.subsample_otutable.output
#     output:
#         expand("mothur_process/{method}/sample.final.0.03.subsample.braycurtis.0.03.lt.pcoa.axes", method="otu_analysis"),
#         expand("mothur_process/{method}/sample.final.0.03.subsample.braycurtis.0.03.lt.pcoa.loadings", method="otu_analysis"),
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash workflow/scripts/pcoa_mds_axes.sh"


# rule nmds_matrix:
#     input:
#         shared=rules.subsample_otutable.output
#     output:
#         expand("mothur_process/{method}/sample.final.0.03.subsample.braycurtis.0.03.lt.nmds.iters", method="otu_analysis"),
#         expand("mothur_process/{method}/sample.final.0.03.subsample.braycurtis.0.03.lt.nmds.stress", method="otu_analysis"),
#         expand("mothur_process/{method}/sample.final.0.03.subsample.braycurtis.0.03.lt.nmds.axes", method="otu_analysis"),
#     conda:
#         "../envs/mothur.yml"
#     shell:
#         "bash workflow/scripts/nmds_axes.sh"


rule render_bs4_book:
    input:
        "index.Rmd",
        expand('{outdir}/{sample}_{readNum}_001.fastq.gz', outdir=OUTDIR, sample=SAMPLES, readNum=config["readNum"]),
        "data/metadata/test.files",
		# "data/references/silva.v4.align",
		# "data/references/trainset16_022016.pds.fasta",
		# "data/references/trainset16_022016.pds.tax",
		# "data/references/zymo.mock.16S.v4.fasta"
    output:
        "docs/index.html"
    shell:
        "R -e 'bookdown::render_book(input = \"index.Rmd\", output_format = \"bookdown::bs4_book\", output_dir = \"docs\")'"

